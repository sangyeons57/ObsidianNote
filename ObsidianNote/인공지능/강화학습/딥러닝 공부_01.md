[유튜브](https://www.youtube.com/playlist?list=PLBiQZMT3oSxW1RS1hn2jWBgswh0nlcgQZ)

퍼셉트론 ~= 뉴런= 노드 과 비슷하다.

페셉트론은 들어오는 전기신호를 전부더해서
전부 더한 값이 임계값 보다 크면 다음 퍼셉트론으로 전기시호를 보네는 신경망의 가장작은 단위이다.
수식으로는
$$x1*w1 + x2*w2 ... \leq 임계값$$
그런데 저기서 임계값을 b(bias) 편향으로 바꿔서 표시한다.
$$x1*w1 + x2 * w2 ... + b \leq 0$$
따라서 이수식을 만족하면 전기신호 (int)1 을 보네고 만족하지 못하면 (int)0 를 보넨다.
따라서 한 퍼셉트론이 활성화 되면 전기신호를 1 보네지만
그 전기 신호를 받는 놈은 w 라는 가중치 를 각가가 곱해서
모든 신호의 합 + 편향 을 해서 또 0보다 크면 활성화 되어 다음 퍼셉트론 에거 영향을 준다.

활성화합수는 크게
Step function 방식과 Sigmoid 방식이 있는데
step function은 0보다작은 모든값은 0, 0보다 큰 모든 값은 1로 하는 함수고
Sigmoid는 
$$\sigma(x) = \frac{1}{1 + e^{-x}}$$
이걸 이용해서 부드럽게 하여 0~1 값을 그리는거다.

추가로 여기에 기울기가 절대값이 커질수록 줄어든다는 문제가 발생하여
ReLU식을 이용한다는데
이건 max(0,value) 인데0보다 작은 값은 0으로 하고 그외값은 그값을 쓰는 그래프이다.
또한 여기에도 문제가 있었는데 아레에서 설명하겠다.

우선 sigmoid방식으로 계속 하자면
저런 방식을 쓰는이유는 미분을 통해 정보를 계속 전달하기 위해서이다.

$$ sigmoid(x1*w1 + x2 * w2 ... + b) $$
이런 느낌이다.
따라서 >= 이걸로 0아니면 1을 반환 하는 형태가 아니라 계속해서 수치를 전달해주는거다.

따라서 Sigmoid는 0으로써 정보를 단절시키는 그런상태가없는거다.
대신 ReLU는 0아레값은 단절 시켜 더이상 학습을 진행시키지 않고 0이상의 값은 계속 학습을 하는 방식을 제안 하는데

여기에서 학습을 단절시키는게 좋을때도 있지만 문제또한 발생되어
세롭게 Leaky ReLU라는 방식이 제안되었다.
Leaky ReLU는 누출 계수를 활용하여 0아레값을 0으로 고정시키느것이아니라
이주 조금식 작계만들어서 그래디언트가 완전히 사라지는것을 방지합니다
이 누출계수는 일반적으로 0.01값ㅇ르 가지게 된다.

Q: 그럼 Leak ReLU는 다음 퍼셉트론으로 정보전달을 안하게 하는것이아니라 부정적인 정보를 전달하여서 ReLU은 장점중 하나가 사라지느거 아냐?

A: 그렇습니다, Leaky ReLU는 음수 입력에 대해 작은 양수 출력을 가지므로, 일종의 ‘부정적인’ 정보를 다음 뉴런으로 전달할 수 있습니다. 이는 기본 ReLU의 특성인 '희소 활성화’를 약화시킬 수 있습니다. '희소 활성화’는 뉴런의 출력이 서로 덜 상관되게 하여, 과적합을 방지하는 데 도움이 됩니다.

그러나 Leaky ReLU를 사용하는 주된 이유는 ‘죽은 뉴런’ 문제를 완화하기 위함입니다. 기본 ReLU에서는 음수 입력에 대해 출력이 0이 되므로, 해당 뉴런은 학습 과정에서 업데이트되지 않게 됩니다. 반면 Leaky ReLU에서는 음수 입력에 대해 작은 양수 출력을 가지므로, 모든 뉴런이 학습 과정에서 업데이트될 수 있습니다.

따라서, Leaky ReLU는 기본 ReLU의 장점 일부를 포기하면서도, ‘죽은 뉴런’ 문제를 완화하는 데 큰 도움이 됩니다. 이는 딥러닝 모델의 성능을 향상시키는 데 중요한 역할을 합니다. 그러나 어떤 활성화 함수가 가장 적합한지는 문제의 종류와 데이터에 따라 달라질 수 있습니다. 따라서, 여러 활성화 함수를 실험해 보고 가장 좋은 결과를 내는 함수를 선택하는 것이 중요합니다.


신경망에서 계산할때 행렬을 사용한다.
그래서 그냥 인풋 * 네트워크 1층 + 편향 1층 하면 1층에 결과값이 행열로 다시 나온다.
1층 값 * 네트워크 2층 + 편향 2층 
이렇게하면 행열의 특수 한 형태로 인해  모든 노드가 자동으로 값 * 가치 + 편향을 각각 계산해준다.

즉, 신경망을 만들고 거기에 파라미터를 조정하는 니낌이아니라
수많은 행렬들의 데이터를 수정하는게 신경망의 느낌이다.


### 손실함수

손실함수는 순방향 계산이 끝났을때 얼만큼의 오차가 있었는지 말해주는 함수다.
일반적으로는 배치를 사용해서 배치양만큼 의 데이터의 오차를 전부 더해서 평균을 내서
학습을 한다. 이는 더 효율적인 학습을 하게 해줄수있다.

손실함수중 유명한건 MSE, CEE등이 있다.
MSE 는 모든 출력노드의 오차의 제곱을 구해 평균을 네어 오차의 크기를 구하는 함수이고
CEE 는로그함수를 이용하여 오차의 크기를 구하는 함수이다.

Q: 그런데 왜 오차의 평균을 내는거야? 가장 큰 오차만 보면 안되? 이러면 오차의 실제크기에비해 오차가 약해 보이는거아냐?
A: 오차의 평균을 구하는 이유는 모델이 전체 데이터에 대해 얼마나 잘 작동하는지를 평가하기 위함입니다. 만약 가장 큰 오차만을 보게 된다면, 그것은 특정 케이스에 대한 모델의 성능만을 반영하게 됩니다. 이는 모델이 전체적으로 얼마나 잘 작동하는지에 대한 정보를 제공하지 못하며, 이상치에 너무 큰 영향을 받을 수 있습니다.

또한, 오차의 평균을 구하면 각 데이터 포인트가 모델의 성능에 동일하게 기여하게 됩니다. 이는 모든 데이터 포인트가 중요하다는 가정을 반영한 것입니다.