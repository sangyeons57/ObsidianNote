DQN = Deep Q Network

DQN은

상태가 너무 많이 이런 매우 많은 상태를 처리하기위해 개발되었고
딥러닝과 Q러닝을 합한 형태이다

사용방식은
Q값 대신 DNN을 사용하여 구하는것인데
Q값에 상태값을 넣어주면 결과 각 액션에 해당하는 값들을넘겨준다

일반적인 딥러닝은
가장 높은 결과값을 판단에 사용하지만

DQN은 각 결과값은 각 액션에 해당한다.
즉,
일반적인 Q-learning은 상태에서 결과를 하고 그 결과값이 큰 것을 골르지만 여기서는 비효율적임으로
DNN의 각 결과 값은 액션과 그로인해 예상되는 기댓값을 나타낸다
Greedy하게 행동할때는 자동적으로 가장 기대값이 높은 행동을 하면 된다.
간단히아자면
액션에따라서 DNN에 각 결과값에 Q 값을 리턴하도록 한거다.

학습하는것은 Q값을 구한는 네트워크를 구현하는거다.

Q-learning은 과거에 행동과 현제의 행동 둘다 따로 학습하지만
DQN에서는 현재 행동은 학습하지않고 과거 행동에대해서만 판단한다.

또한 학습에서 연결성이 없는 개별 정보로는 판단이 어려워

여러개의 정보를 가지고 판단한느데
이때 과하게 가까운 정보를 가지고 사용하면 학습을을 잘 못함으로

이정량 중에서랜덤으로 몇개를 골라서 학습한다.


DNN은닉층
DNN(Deep Neural Network)에서 은닉층의 뉴런 개수와 층의 개수는 모델의 복잡성과 성능에 중요한 영향을 미칩니다.

**뉴런의 개수**: 각 은닉층의 뉴런 개수는 그 층이 표현할 수 있는 복잡성을 결정합니다. 뉴런이 많을수록 모델은 더 복잡한 패턴을 학습할 수 있지만, 과적합(overfitting)의 위험이 증가합니다. 과적합은 모델이 학습 데이터에 너무 잘 맞아서 새로운 데이터에 대한 예측 성능이 떨어지는 현상을 말합니다.

**층의 개수**: 층이 많을수록 DNN은 더 복잡하고 추상적인 특징을 학습할 수 있습니다. 예를 들어, 이미지 분류 문제에서 초기 층은 엣지나 질감 같은 저수준 특징을, 중간 층은 눈이나 코 같은 중간 수준 특징을, 마지막 층은 얼굴이나 객체 같은 고수준 특징을 학습할 수 있습니다. 그러나 층이 너무 많으면 모델의 학습이 어려워질 수 있고, 과적합의 위험이 증가할 수 있습니다.

따라서, 뉴런의 개수와 층의 개수는 적절히 선택해야 합니다. 이는 특정 문제와 데이터에 따라 다르며, 일반적으로는 교차 검증(cross-validation) 등의 방법을 사용하여 최적의 값을 찾습니다. 이는 딥러닝에서 중요한 하이퍼파라미터 튜닝 과정의 일부입니다.


Q: 그럼 내가 맞는지 확인 해줘 행동 (순전파) 는 매 프래임 마다 반복하고, 학습(역전파) 는 일정 프레임 마다 반복하는데 학습은 행동때 들어온 값들중 랜덤으로 몇개만 뽑아서 학습한다는거지?

A: 네, 맞습니다! DQN(Deep Q-Network)에서는 매 타임스텝마다 액션을 선택하고 수행하는데, 이는 신경망의 순전파(forward pass)를 통해 이루어집니다. 그리고 일정한 간격으로 재생 버퍼에서 무작위로 일련의 경험을 샘플링하고, 이를 사용하여 신경망을 학습시키는데, 이는 신경망의 역전파(backward pass)를 통해 이루어집니다. 따라서, 매 타임스텝마다 순전파가 발생하며, 일정한 간격으로 역전파가 발생합니다. 이것이 바로 DQN에서 사용되는 강화학습의 핵심 원리입니다. 😊

재생 버퍼에 수백만개의 데이터를 저장하고 그중 32개를 랜덤으로뽑아서 쓴다는 말,
데이터의 관련성이 필요해서 최근 데이터만 들고있을 줄알았는데,
그럴필요 없이 최대한 다양한 경험에 대해서 계속해서 학습 시켜주는거싱 목적이다.
Q : 그러면 그렇게 많은 수백만개 를 전부 재생 버퍼로하면 너무 관련성이 떨어지는거아냐? 데이터끼리 관련성이 있을 필요는 없나?

A: 재생 버퍼(replay buffer)에 많은 수의 경험을 저장하는 것은 DQN(Deep Q-Network) 학습의 중요한 부분입니다. 이는 에이전트가 다양한 상황에서 학습할 수 있도록 하기 위한 것입니다. 그러나, 이것이 모든 경험이 동등하게 중요하다는 것을 의미하지는 않습니다.

실제로, 일부 경험은 다른 경험보다 더 중요할 수 있습니다. 예를 들어, 특정 액션을 취했을 때 큰 보상을 얻거나, 특정 상태에서 새로운 액션을 발견했을 때 등입니다. 이러한 중요한 경험을 더 자주 학습하도록 하는 것이 유익할 수 있습니다.

이를 위해 "우선 순위 기반 경험 재생(Prioritized Experience Replay)"이라는 기법이 제안되었습니다. 이 기법은 각 경험의 중요성(우선 순위)을 계산하고, 중요한 경험을 더 자주 샘플링하도록 합니다. 이는 학습 과정을 더 효율적으로 만들고, 성능을 향상시킬 수 있습니다.

따라서, 재생 버퍼에 많은 수의 경험을 저장하는 것이 중요하지만, 이는 모든 경험이 동등하게 중요하다는 것을 의미하지는 않습니다. 대신, 중요한 경험을 더 자주 학습하도록 하는 것이 중요합니다. 이는 강화학습에서 중요한 연구 주제 중 하나입니다. 😊
