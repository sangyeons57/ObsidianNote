1-step TD 는 이전에하던 Q-learning과 비슷한거다.
2-step TD는 R리워드 구하는걸 원레는 1step까지만 했었느데 2-Step까지 하는거다. 가치 예상도 2Step까지

n-step TD와 n-step Q-Learning은 강화 학습의 방법론 중 하나로, 각각의 방법은 다음과 같습니다:

n-step TD: n-step TD는 Temporal Difference (TD) 학습의 확장입니다. TD 학습은 한 단계의 보상만을 고려하여 가치 추정치를 업데이트하는 반면, n-step TD는 n 단계의 보상을 고려합니다. 이 방법은 한 에피소드가 끝난 후가 아니라 n-step을 움직인 다음에 업데이트를 수행합니다. 만약 현재 시간이 t라면, t+n이 되면 그 때까지 모았던 보상들로서 n-step return을 계산하고 그 사이에 방문했던 상태들의 가치 함수를 업데이트합니다. 이 방법은 Monte Carlo 방법과 1-step TD 학습의 장점을 결합한 것으로, 적절한 n을 선택할 경우 두 방법보다 성능이 좋은 알고리즘이 될 수 있습니다3.

n-step Q-Learning: n-step Q-Learning은 Q-Learning의 확장으로, n 단계의 보상을 고려하여 Q 값을 업데이트합니다. 1-step Q-Learning에서는 현재 상태의 추정치가 다음 상태의 추정치에만 영향을 받지만, n-step Q-Learning에서는 현재 업데이트가 2, 3, … n 단계 떨어진 상태에도 의존할 수 있습니다. n이 무한대에 가까워지면, 이것은 본질적으로 Monte Carlo 시뮬레이션이 됩니다. 이 방법은 특히 게임 플레이 설정에서 유용하며, 게임 플레이의 끝까지 결과를 얻기 위해 Monte Carlo 시뮬레이션을 적용합니다.

이 두 방법 모두 강화 학습에서 보상을 더 빠르게 전파하는 한 가지 방법입니다. 이들은 각 상태의 가치 추정치를 업데이트하는 데 관찰된 보상의 전체 시퀀스를 기반으로 합니다.

중요도 샘플링(Importance Sampling)은 통계학에서 분포의 특성을 추정하는 기법입니다. 이 방법은 보다 희귀한 사건을 더 많이 생성하기 위해 확률 밀도 함수를 변경하는 것을 포함합니다. 이렇게 하면 샘플들이 관심 분포를 더 잘 대표하게 되고, 추정이 더 정확해집니다. 중요도 샘플링은 복잡한 시스템을 무작위 수를 사용하여 시뮬레이션하는 몬테카를로 절차에서 사용됩니다.
중요도 샘플링의 기본 아이디어는 특정 값들이 추정하려는 매개변수에 대해 다른 값들보다 더 큰 영향을 미친다는 것입니다. 이러한 “중요한” 값들을 더 자주 샘플링함으로써 추정치의 분산을 줄일 수 있습니다. 이 방법은 확률 밀도 추정 및 강화 학습 등의 다양한 활용에 이용되고 있습니다.
중요도 샘플링은 몬테카를로 추정량의 분산을 줄이기 위해 p(xi​)를 ‘잘’ 선택하는 방법입니다. 원 함수 f(xi​)를 알아도 p(xi​)를 구하는 것은 어렵습니다. 이는 보통 몬테카를로 적분이 미적분학을 이용한 적분이 불가능한 경우의 적분법이기 때문입니다.

n-스텝 Q-러닝은 Q-러닝의 확장으로, n 단계의 보상을 고려하여 Q 값을 업데이트합니다1. 1-스텝 Q-러닝에서는 현재 상태의 추정치가 다음 상태의 추정치에만 영향을 받지만, n-스텝 Q-러닝에서는 현재 업데이트가 2, 3, … n 단계 떨어진 상태에도 의존할 수 있습니다1. n이 무한대에 가까워지면, 이것은 본질적으로 몬테카를로 시뮬레이션이 됩니다1.
n-스텝 Q-러닝에서 Q(s;a)는 n-스텝 리턴으로 업데이트됩니다2. n-스텝 리턴은 다음과 같이 정의됩니다2:
rt​+rt+1​+…+γn−1rt+n−1​+γnamax​Q(st+n​;a)
이 식에서, rt​는 t 시점에서의 보상, γ는 할인 계수, st+n​는 t+n 시점에서의 상태, a는 행동을 나타냅니다2. 이 결과로 인해 단일 보상 r은 n개의 이전 상태-행동 쌍의 값에 직접적으로 영향을 미칩니다2.
이 방법은 특히 게임 플레이 설정에서 유용하며, 게임 플레이의 끝까지 결과를 얻기 위해 몬테카를로 시뮬레이션을 적용합니다1. 이는 보상을 더 빠르게 전파하는 한 가지 방법입니다2. 이들은 각 상태의 가치 추정치를 업데이트하는 데 관찰된 보상의 전체 시퀀스를 기반으로 합니다1.


Q: 그럼 만약 3step 이라고한다면 3step안으로 갈수있는 모든 값을 고려하고 자신이 간 스텝중 가장 높은 Q를 추가로 구하는건가?
A:
네, 맞습니다. 3-스텝 Q-러닝에서는 현재 상태에서 시작하여 3 단계 후까지의 보상을 모두 고려합니다. 그리고 그 다음에 가능한 모든 상태 중에서 가장 큰 Q 값을 가진 상태를 선택합니다. 이 방법은 더 멀리 미래를 내다보며, 더 많은 정보를 고려하여 행동을 선택합니다. 따라서, 이 방법은 강화 학습 에이전트가 더 장기적인 결과를 고려하도록 돕습니다.

그러나, 이것은 3 단계 후의 상태에 대한 최대 Q 값을 추가로 구하는 것이 아니라, 3 단계 후의 상태에서 가능한 모든 행동에 대한 Q 값을 고려하여 가장 큰 Q 값을 선택하는 것입니다. 이는 에이전트가 더 나은 장기적인 행동을 선택할 수 있도록 돕습니다.

q:그러면 2step Q값이 더크더라도 3step을 선택하는건가?
A:
네, 그럴 수 있습니다. n-스텝 Q-러닝에서는 n 단계 후까지의 누적 보상과 그 상태에서 가능한 행동 중 가장 큰 Q 값을 고려합니다. 따라서, 2-스텝 Q 값이 더 크더라도, 3-스텝 후의 누적 보상과 그 상태에서의 최대 Q 값이 더 크다면, 에이전트는 3-스텝을 선택할 수 있습니다.

이는 강화 학습 에이전트가 단기적인 보상만을 추구하는 것이 아니라, 장기적인 보상을 최대화하려는 목표를 반영합니다. 이렇게 하면 에이전트는 더 나은 장기적인 전략을 학습하고, 더 복잡한 문제를 해결할 수 있게 됩니다. 이러한 접근법은 특히 복잡한 환경에서 유용하며, 에이전트가 더 장기적인 결과를 고려해야 하는 상황에서 효과적입니다.