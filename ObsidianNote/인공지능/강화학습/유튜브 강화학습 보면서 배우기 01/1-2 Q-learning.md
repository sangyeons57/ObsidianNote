[강화학습 1-2 강의 유튜브](https://youtu.be/3Ch14GDY5Y8?si=EmNjbvOL8mLiPTYL)
Reinforcement learning =  강화학습

Reinforcement learning은 맛집 찾기와 같다.

격자 형태의 지도에서
시작 지점과 맛집이 있으면

시작 지점에서 맛집으로 가기 위한 방법을 계속 해서 시뮬레이션 하는 것 이다.

Greedy action = 최대 이익 방식

격자를 4방향 = 상 하 좌 우 로 나누었을때

각 격자는 하나의 상태 Status를 나타낸다

처음 status 는 4 direction 이 모두 reward가 0 이다.

따라서 4방향 모두 랜덤으로 어딜갈지 정한다.

이걸 계속 반복하다 어쩌다 맛집에 들어간경우
맛짐으로 들어가기 status에서 맛집으로 유도한 action에 reward를 준다.

이걸계속 반복하다보면 언젠가 맛집으로 들어간 경험이 있는 Status를 만나면
해당 Status의 최대 값 (action의 값들중 현제는 action이 4방향 4개 임) 을
해당 status로 유도하는 action에 값 추가한다.

이걸 계속 반복하면 언젠가 시작 지점부터 맛집 까지 가는길에 모든 statsu가
값을 보유하게 되는데

그러면 시작점 도 status임으로 시작점의 방향(action) 까지 정해지면
처음에는 랜덤이였지만 이 상황에서는
100이면 100 전부 해당 동일한 방법으로 맛집에 가게 될거다.

하지만 이런 Greedy action에서는 더좋은 방법이나
다른 더큰 Reward를 발견하지 못할 가능성이 있다.

따라서 Exploration(탐험)이라는 걸 한다.
그래서 이 탐험을 추가한 방법이 입실론-Greedy 이다.

ES = 입실론 이라고 할때 (0 <= ES <= 1)

ES = 1 이라고하면
action의 값과 관계 없이 완전히 랜덤으로 움직이거
ES = 0 이라고 하면
탐험은 전혀 하지 않고 정해짖 action최대치 (Greedy)하게 움직이게 된다.

거기에 추가로 입실론 그리디 방식은
Exploration(탐험)을 에피소드 가 진행함에 때라 점점 줄이고
Exploitation(개척)을 점점 늘린다.

하지만 이것들만 있는경우
Path(방법) 에 대한 가치의 차이 가 없어진다
즉, 한가지 목표를 위해 가능 방법중 더 효울적인 방법과 그렇지 않은 방법의 차이를 알수없다느거다.
따라서 여기에 Discount factor 를 추가한다.

G = 감마(0 <= G <= 1)

감마는 이런 방식으로 작동한다
Status 중 Reward를 가진 놈은 자신 Status Action에 값을 줄때 감마를 곱해서 추가하는거다.
즉, Reward = 1이라고하면
S1 = 1
S2 = 1 * G      = G
S3 = 1 * G^2 = G^2
이런 식이다 G = (0~1) 사이 임으로 
0에 가까울수록 더 먼 목표도 가치를 높게 부여하겠다느것이고
1에 가까울수록 가까운 목표에 가치를 높게 부여하겠다는 것이다.

또한 먼 길을 돌아갈수록 G가 추가로 곱해짐으로 해당 action의 가치가 떨어진다.

추가로 학습 곡선이 있는데 식은 이렇다
Q(StAt) <- (1 - a)Q(StAt) + a(Rt + G * maxQ(S(t+1) A(t+1)))

a= 학습 비율
t= 시간이다
Rt = 는 Reward
Q(StAt) = 해당 Status의 가치

a가 0 에 가까울수록 Q(StAt) {현재값} 을 유지하고
a가 1에 가까울수록 max Q(S(t+1) A(t+1)) {세로운값} 을 더 받아들인다.