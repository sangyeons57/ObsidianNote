State Value function: 지금 이 state에서 기대되는 리턴을 계산하는것이고
지금 상태의 가치
Action Value function: 은 지금 state에서 action을 할때 기대되는 리턴을 계산하는것이다.
지금 행동의 가치

Optimal Policy : 과거는 생각 하지않고 현제 State에서 최대 리턴을 추구하는것이다 (State value function)

즉 state Value Functiono 을 maximize하느것이 optimal policy이다.

MDP에서 봤듯이 현재 가치return 를 계산하는 방식이있다

State Value function
현제 상태에서 할수있는 모든 행동을 다한것의 적분 현재 상태의 가치가 된다.

Q: 그럼 state value function는 특정 상태에서 action을 했을때 기대하는 가치만 보는거야? 다음 state에서 action의 가치는 보지않고?

A: 아니요, 상태 가치 함수는 특정 상태에서 시작하여 **정책 π를 따르는 예상 반환값**을 나타냅니다. 이는 특정 상태에서의 행동만을 고려하는 것이 아니라, 그 행동으로 인해 도달할 수 있는 모든 가능한 미래 상태와 그 상태에서 취할 수 있는 행동들의 가치도 고려합니다.

상태 가치 함수는 현재 상태에서의 행동에 대한 보상뿐만 아니라, 그 행동으로 인해 도달하는 다음 상태에서의 행동에 대한 보상들을 할인 요인 γ를 고려하여 더한 값입니다. 따라서 상태 가치 함수는 현재 상태뿐만 아니라 미래 상태에서의 행동들의 가치도 고려합니다.

다만, 상태 가치 함수는 특정 상태에서의 모든 가능한 행동들의 가치를 평균내는 반면, **행동 가치 함수**는 특정 상태에서 특정 행동을 취했을 때의 가치를 직접적으로 나타냅니다. 행동 가치 함수는 상태 가치 함수와 달리 특정 행동에 대한 정보를 제공하므로, 행동 선택에 더 직접적으로 사용될 수 있습니다.

Action Value Function

A: 강화 학습에서 **행동 가치 함수**는 특정 상태에서 특정 행동을 취했을 때의 기대 가치를 나타냅니다. 행동 가치 함수는 보통 Q로 표기되며, 정책 π를 따르는 상태 s에서 행동 a를 취했을 때의 예상 반환값을 나타냅니다
행동 가치 함수는 다음과 같이 표현될 수 있습니다:

Qπ(s,a)=Eπ​[Gt​∣St​=s,At​=a]

여기서,

- Qπ(s,a)는 상태 s에서 행동 a를 취하고 정책 π를 따르는 행동 가치 함수를 나타냅니다.
- Eπ​[Gt​∣St​=s,At​=a]는 상태 s에서 행동 a를 취하고 정책 π를 따를 때의 예상 반환값을 나타냅니다.
- Gt​는 시간 단계 t에서의 총 누적 보상을 나타냅니다.

행동 가치 함수는 상태 가치 함수와 달리 특정 행동에 대한 정보를 제공하므로, 행동 선택에 더 직접적으로 사용될 수 있습니다. 최적의 행동 가치 함수를 알고 있다면, 그것만으로도 최적으로 행동하는 방법을 알 수 있습니다


Q: 그런데 특상 상태에 행동을 했을때 가치를 구하는거면 그냥 다음 state의 가치를 구하는것과 같은거 아냐?

A: 아니요, 상태 가치 함수와 행동 가치 함수는 서로 다릅니다.

**상태 가치 함수** Vπ(s)는 특정 상태 s에서 시작하여 정책 π를 따를 때의 예상 반환값을 나타냅니다. 이는 현재 상태에서의 행동에 대한 보상뿐만 아니라, 그 행동으로 인해 도달하는 모든 가능한 미래 상태와 그 상태에서 취할 수 있는 행동들의 가치도 고려합니다.

반면에 **행동 가치 함수** Qπ(s,a)는 특정 상태 s에서 특정 행동 a를 취했을 때의 기대 가치를 나타냅니다. 이는 현재 상태에서의 행동에 대한 보상과 그 행동으로 인해 도달하는 다음 상태에서의 최적 행동에 대한 보상을 고려합니다.

따라서 행동 가치 함수는 특정 상태에서 특정 행동을 취했을 때의 가치를 직접적으로 나타내는 반면, 상태 가치 함수는 특정 상태에서의 모든 가능한 행동들의 가치를 평균내는 차이가 있습니다.

즉 상태 가치함수는 = 미래상태와 미래상태에서 취할수있는 모든것의 가치를 구하지만
행동 가치함수는 = 현제상태에서 이 행동을 할때의 가치만 구한다.
