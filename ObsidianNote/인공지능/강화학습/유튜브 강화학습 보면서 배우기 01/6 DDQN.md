우선 DDQN은 과대평가문제를 해결하기위해 나왔다.
과대평가 문제는 불확실성이 높은 상태에서 특정행동을 과도하게 평가하는 문제이다

이걸 해결하기 위해 
$$Q(s, a) \leftarrow Q(s, a) + \alpha [r + \gamma \max_{a'} Q(s', a') - Q(s, a)]$$
식을

$$Q(s, a; \theta) \leftarrow Q(s, a; \theta) + \alpha [r + \gamma Q(s', \arg\max_{a'}Q(s', a'; \theta), \theta^-) - Q(s, a; \theta)]$$
이렇게 바꾼다
즉,
현재 행동을 평가할때(q값)
과거 Q의 다음 스테이트 최대 기대값 + 보상 을 Q값으로 정했는데

현재 Q값을 최대로 하는 액션을 과거 Q값에 계산해서 행동한다 

A: 네, 맞습니다. Double DQN에서는 한 번의 업데이트를 위해 신경망을 세 번 순전파합니다. 첫 번째 순전파는 현재 상태에서 행동을 선택하는 데 사용되며, 두 번째 순전파는 선택한 행동의 가치를 추정하는 데 사용됩니다. 마지막으로, 세 번째 순전파는 손실 함수를 계산하고, 이를 통해 신경망의 가중치를 업데이트하는 데 사용됩니다.

A: 네, 맞습니다. Double DQN의 목표는 신경망이 추정한 Q 값과 실제 Q 값 사이의 차이, 즉 손실을 최소화하는 것입니다. 이 손실은 다음과 같은 손실 함수로 계산됩니다:

$$L(w) = \mathbb{E} [(r + \gamma Q(s', \arg\max_{a'}Q(s', a'; \theta), \theta^-) - Q(s, a; \theta))^2]$$

여기서 $\mathbb{E}$는 기대값(expectation)을 의미합니다. 이 손실 함수를 최소화하기 위해 경사하강법(gradient descent) 등의 최적화 알고리즘을 사용합니다. 따라서, Double DQN의 목표는 이 손실을 최소화하여, Q 값이 더 정확하게 추정되도록 하는 것입니다.

알았다
여기서 앞에 Q(Target Q) 는 다음 스테이트에서 가장 가치 높은 액션을 가지고오는건데
만약 
여기석 r(dicount factor) 
이 1 일 경우 그 가치를 시간에 따른 감소 없이 그대로 가져와서 현제 Q값과 비교한다는것임으로
가치를 조금 하락시켜서 비교한후
그거를 제곱하여서 맞추는거다.

아 , 알고있었는데 너무 돌려생각해서 복잡해진거같다.
