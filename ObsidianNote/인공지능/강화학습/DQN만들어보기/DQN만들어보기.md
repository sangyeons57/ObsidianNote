
우선 어디서 부터 시작할지 생각해야한다.
DQN을 생각해보면
우선 Replay Memory 가있다.
무작위 로행동을 하되 행동들을 저장하고
나중에 신경망에 학습해야한다.
저장해야하는 종류는 상태, 행동, 보상, 다음상태 이다.

그럼 여기서 학습할때는 무작위로 버퍼에서 추출해서
상태에 따른 결과를 보고 행동을 선택한다음
다음 상태에 따를 결과중 최적의 값을 계산해서 감마와 곱하고 리워드랑 더해서 손실오차를 구하게된다.

그럼우선 가장 기본이 되는 Q 신경망 부터 만들어야겠다.

부분을 나눈다면
인풋을 처리하는부분
레이어 구현하는부분
출력처리 하는부분 일거 같은데

레이어 구현 이라
레이어는 가중치와 노드로 만들어 져있다하면
필요한값들은
노드개수 정도가 될려나?
가중치의 개수를 정하기위해서는 이전 노드의 개수를 알아야하는데

신경망 클래스를 만들고
거기에다가 레이어를 추가하는 함수를 만들자
**Accord.NET:** 이걸이요해서 행열 연산을 하고
순전파구현을해보자
그다음 역전파를 만들고
