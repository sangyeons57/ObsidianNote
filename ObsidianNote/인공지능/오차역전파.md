[유튜브](https://youtu.be/tIeHLnjs5U8?si=9gYH7_Kg7lLuSKyM)
오차역전파란 인공신경망이 학습을 하기 위한 방식이다.
기본적으로 알아야할것은 순전파의 작동방식 편미분 체인룰 정도가 되겠다.

오차역전파는 결과 에서 나오 오차의 크기를 역으로 전파하는 것인데.
오차의 양만큼만 전파하는것이 아니라 오차에 미친 영향만큼 전파하는것이다.

우선 기본 아이디어는 체인룰을 이용하여 오차율을 미분을 이용하여 전파하는것인데

출력부에서 결과 가 나오면 실제값과 결과가 나온 예측 값을 비교한다
출력값 a(L)
실제값 y

y - a(L)
하지만 이러면 오차가 -, + 가 나와 실제 오차가있지만 없어지는것처럼 보여 제곱을 해주게된다.
그리고 이것을 Loss 손실함수 =C 라고한다.

$$Loss = C = (y - a(L))^2$$
여기서 a는 활성화 함수라고 할수있다 여기서는 시그모이드 함수라고 생각한다.
그리고 현제 노드의 값을 분해 해본다.
$$ z(L) = w(L)a(L-1) + b(L) $$
z 는 가중치 * 이전 노드의값 + 편향 이다

그러면
a\`을 시그모이드 미분값이라고한다면
a\` = 시그모이드 미분(Z(L))
$$ {\delta a(0) \over \delta z(L)} = a`(z(L)) $$
$$ z(L) = w(L)a(L-1) + b(L) $$
위식을 이용해서 z에 대한 w의 미분을구한
$$ {\delta z(L) \over \delta w(L)} = a(L-1) $$

이라고 할수있다.

$$ {\delta C(0) \over \delta w(L)} =  {\delta z(L) \over \delta w(L)}  {\delta a(L) \over \delta z(L)}  {\delta C(0) \over \delta a(L)}  $$

이것이 체인룰을이용한  Loss =C에 loss를 계산한 값이다.

위식을 위에 값들로 풀면
$$ {\delta C(0) \over \delta w(L)} =  {\delta z(L) \over \delta w(L)}  {\delta a(L) \over \delta z(L)}  {\delta C(0) \over \delta a(L)} = {a(L-1) a`(z(L))2(a(L) - y)} $$
가된다 따라서 가중치의 미분값은 = 이전노드  x 미분시그모이드값 x  2 x 오차의 미분값 이라고 할수있다.

$$ {\delta C(0) \over \delta b(L)} =  {\delta z(L) \over \delta b(L)}  {\delta b(L) \over \delta z(L)}  {\delta C(0) \over \delta a(L)} = {1 \times a`(z(L))\times2(a(L) - y)} $$
위식은 편향의 미분으로 편향은 위와같이 미분시 그모이드값 x  2 x 오차 미분값 이다

위에 식의 가중치를 구하는 식처럼 a(L-1) 을 구하느 식은 a(L-1) 대신 w(L) 을 곱하면 구할수있다.

그러면 이걸계속 반복해서 위식까지 전부 오차 역전파를 할수있는것이다.


이걸 간단히 하면
오차를 역전파 할때는 역으로 계산하면된다
오차율 미분 ->
{
활성화 함수미분 ->
모든 노드 값의 합(가중치1 * 노드1 + 가중치2 * 노드2 ... + 편향) ->
하지만 모두 합한것임으로 영향력은 모두같음으로 loss는 각각 노드가 같음 = 가중치 * 노드 ->
가중치는 모든 이전미분에 곱한것에 이전 노드를 곱한것,
이전노드는 Loss 는 모든 이전미분에 곱한것에 가중치를 곱한것,
편향은 모든 이전 미분 곱한것

}
->이걸 계속 반복하는거다.

오차를 계산할때는
각 출력 노드의 오차를 따로 계산한다.
이건 모든 신경망이 같다
손실함수를 이용하여 오차가 어는정도인지 계산은 하나 실제로 역전파되는값은 아니다.

A: 네, 각 노드에서의 손실은 해당 노드의 출력에 대한 손실 함수의 편미분 값으로 계산됩니다. 이를 수식으로 표현하면 다음과 같습니다:

각 노드 $i$에서의 손실 $\delta_i$는 다음과 같이 계산됩니다:

$$\delta_i = \frac{\partial L}{\partial y_i} = y_i - t$$

여기서 $L$은 전체 손실 함수이고, $y_i$는 노드 $i$의 출력입니다. 이 편미분 값은 노드 $i$에서의 오차를 나타내며, 이 오차는 역전파 과정에서 가중치를 업데이트하는 데 사용됩니다.

따라서, 각 노드에서의 손실은 해당 노드의 출력에 대한 손실 함수의 편미분 값으로 계산되며, 이 오차는 역전파 과정에서 가중치를 업데이트하는 데 사용됩니다.

위에값은 MSE에 경우 에서의 계산이다. 여기서는 예측값 - 타겟 값으로 계산된다

하지만 크로스엔트로피 방식에서는
$$L = -\sum_{i=1}^{2} t_i \log y_i$$
이걸 편미분하면
$$\frac{\partial L}{\partial y_i} = -\frac{t_i}{y_i}$$
이게된다 즉 -(타겟값 / 예측값) 이 된다.

여기서 -(마이너스) 가 붙은 이유는 로그 함수를 뒤집기위해 -를 곱한거다.
따라서 실제 표현값은 양수 혹은 0 이된다.

q: 학습률을 처음 오차에 적용하지않고 각각의 요소에서 곱해주는이유
A: 처음 오차에서 학습률을 적용하면, 그래디언트의 크기가 일정하게 유지되지 않을 수 있습니다. 왜냐하면 각 가중치에 대한 그래디언트는 가중치의 현재 값과 입력 값, 그리고 오차에 모두 의존하기 때문입니다. 따라서, 학습률을 처음 오차에 적용하는 것은 가중치 업데이트를 제대로 수행하지 못할 수 있습니다.


$$\frac{\partial L}{\partial w} = \frac{1}{m} \sum_{i=1}^{m} \frac{\partial L_i}{\partial w}$$
위식은 미니배치를 활용하여 각 요소 의 오차의 평균을 구한 식이고 

$$\Delta w = -\eta \frac{1}{m} \sum_{i=1}^{m} \frac{\partial L_i}{\partial w}$$
위식은 그 위에 식에 학습률을 곱한것이다.
이식은 가중치의 변화량을 구하는 식이기때문에 - 가 들어가고
그리고 1번째식은 기울기를 구한것이고 기울기는 손실이 증가하는 방향을 구한것이다.

따라서 손실을 감소시키기위해서는 2번째 식처럼 -를 추가하게된다.
$$w_{\text{new}} = w_{\text{old}} - \Delta w$$
그리고 업데이트를 할때 즉 세로은 요소를 만들때에는 
원레 값 - 오차율 을 해주게된다. 즉 식은
$$w_{\text{new}} = w_{\text{old}} - (-\eta \frac{1}{m} \sum_{i=1}^{m} \frac{\partial L_i}{\partial w})$$
이렇게되고 이식은 결국
$$w_{\text{new}} = w_{\text{old}} + \eta \frac{1}{m} \sum_{i=1}^{m} \frac{\partial L_i}{\partial w}$$
이렇게 표현할수있다.

