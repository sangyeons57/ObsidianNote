가중치 초기화가 중요한 이유는 가중치가 모드 같은 경우 모든 노드가 같은 출력을 보내기때문에 학습하기어렵다 따라서 랜덤을 사용해야하는데 
어떤랜덤을 사용하냐가 가중치 초기화의 중점이다.

크게
1. **무작위 초기화** : 무작위 값 할당
2. **Xavier 초기화:** 이전 레이어의 뉴런수를 고려하여 출력의 분산을 유지하도록 가중치를 초기화하는방법
3. **He초기화:** ReLU 활성화 함수를 사용하는 노드에 대한 가중치 초기화 전략이다.

Xavier초기화 는 특정분포에서 랜덤을 추출하는 방식인데
x 에서 -x 사이 값을 랜덤을 추출하는데
종류는 Uniform 과 Normal이 있는데
Uniform: sqrt(6 / (fan_in + fan_out))
Normal: sqrt(2 / (fan_in + fan_out))

여기서 fan_in: 가중치 탠서에 입력 유닛수 / fan_out: 출력 유닛수 이다.



편향(bias) 는 일반적으로 0 으로 초기화 하지만
하지만 마지막 레이어는(출력층) 특변한데 
예를 들어 평균이 50 인경우 최종평향을 50 으로 초기화 해야한다.

