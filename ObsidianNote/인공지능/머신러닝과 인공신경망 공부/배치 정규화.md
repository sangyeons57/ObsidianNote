배치 정규화란
[유튜브](https://youtu.be/m61OSJfxL0U?si=ekJcfw8OdHphJxwk)
[유튜브](https://youtu.be/daDQUBTISVg?si=KR0FxL8wM75S9yV6)
장점
1. 학습 안정성 향상
2. 학습 속도 향상
3. 과적합 억제
4. 초기화에 덜 민감
단점
1. 계산 비용증가
2. 테스트시 변동성
3. 층간 의존성
4. 순서의존성

또한 ReLU 에서는 기울기 소실이 없기때문에 일반적으로 의미는 없으나
하지만 학습을 안정화 시키기위해 필욯나 경우도 있다.
ReLU에서는 안하는게 좋을거 같기는하단 리소스도 많이 사용하고 큰효과도 없어

추가적으로 배치정규화 와 레이어 정규화 모드 DQN에서는 의미 가 없거나 쓸수없다.

배치정규화의 과정
1. **미니배치 내에서 각 피처(특성)의 평균과 분산을 계산합니다.**
2. **배치 정규화 수식을 사용하여 입력을 정규화합니다.**
3. **정규화된 값을 활성화 함수에 입력으로 전달합니다.**
4. **가중치와 편향을 사용하여 정규화된 값을 조정합니다.**

1. **평균 (mean):**
   $$\mu = \frac{1}{m} \sum_{i=1}^{m} x_i $$

2. **분산 (variance):**
   $$ \sigma^2 = \frac{1}{m} \sum_{i=1}^{m} (x_i - \mu)^2 $$

3. **정규화 (normalize):**
$$ \hat{x}_i = \frac{x_i - \mu}{\sqrt{\sigma^2 + \epsilon}} $$

   여기서 \(\epsilon\)은 분모가 0이 되는 것을 방지하기 위한 작은 상수입니다.

4. **스케일(scale) 및 시프트(shift) 조정:**
  $$ y_i = \gamma \hat{x}_i + \beta $$

   여기서 \(\gamma\)는 스케일 파라미터(가중치), \(\beta\)는 시프트 파라미터(편향)입니다.

즉 모든 입력과 가중치를 곱한값 과 편향을 더한값에
스케일을 곱하고 다시 시프트 편향을 더하는 방식이다
그다음 여기다가 활성화함수를 적용하면 그이후로는 똑같다.




다음은 간단한 배치 정규화의 역전파 수식입니다. 여기서 \(\frac{\partial L}{\partial \hat{x}_i}\), \(\frac{\partial L}{\partial \gamma}\), 그리고 \(\frac{\partial L}{\partial \beta}\)를 계산하는 것이 목표입니다. \(L\)은 손실(비용) 함수입니다.

1. **그라디언트 계산 (미니배치에 대한):**
   $$ \frac{\partial L}{\partial \hat{x}_i} = \frac{\partial L}{\partial y_i} \cdot \gamma $$
   $$ \frac{\partial L}{\partial \gamma} = \sum_{i=1}^{m} \frac{\partial L}{\partial y_i} \cdot \hat{x}_i $$
   $$ \frac{\partial L}{\partial \beta} = \sum_{i=1}^{m} \frac{\partial L}{\partial y_i} $$

2. **그라디언트를 사용하여 입력에 대한 그라디언트 계산:**
   $$ \frac{\partial L}{\partial x_i} = \frac{\partial L}{\partial \hat{x}_i} \cdot \frac{1}{\sqrt{\sigma^2 + \epsilon}} - \frac{\partial L}{\partial \gamma} \cdot \frac{\sum_{i=1}^{m} 2(x_i - \mu)}{m(m-1) \sqrt{\sigma^2 + \epsilon}} - \frac{\partial L}{\partial \beta} \cdot \frac{1}{m} $$


4. **선형 변환의 역전파 (Backward Pass for Linear Transformation):**
   $$ \frac{\partial L}{\partial Z} = \frac{\partial L}{\partial \hat{X}} \cdot \frac{1}{\sqrt{\sigma^2 + \epsilon}} - \frac{\partial L}{\partial \gamma} \cdot \frac{\sum 2(Z - \mu)}{m(m-1) \sqrt{\sigma^2 + \epsilon}} - \frac{\partial L}{\partial \beta} \cdot \frac{1}{m} $$
   weight
   $$ \frac{\partial L}{\partial W} = X^T \cdot \frac{\partial L}{\partial Z}$$ 
   bias
   $$ \frac{\partial L}{\partial b} = \sum \frac{\partial L}{\partial Z}$$
