활성화 함수를 간단하게 필요한 이유를 말하자면
신경망에서 비 선형적인 데이터를 처리하기 위해서 필요하다.

활성화 함수가 없으면 모든 레이어가 선형적으로 데이터를 처리한다.
예를 들어
인풋이 a가 있고 절편이 b고 3번의 퍼셉트론을 거친다고 했은때 활성화 함수가 없으면

```
((a * w(1,1) + b(1) + w(1,2)) * w(2,1) + b(2) * w(2,2)) * w (3,1) + b(3) * w(3,2)
```
위처럼 신경망이 계산 되어 결과가 나오는데

이렇게 되면 Deep하게 되도 결국 한개있는거랑 다르지 않게 y = x 그래프 처럼 들어온 값을 그대로 뱉어서 비선형 적인 데이터를 처리하지 못하게 된다.

따라서 활성화 함수를 통해서 결과의 적용과 미적용의 여부를 결정해서 AND, OR, XOR게이트 등을 포함해 더 본잡한 계산이나 분류도 구현 할수있게 됩니다.

## 1. Sigmoid 함수

- **식**: $$f(x) = \frac{1}{1 + e^{-x}}$$
- **출력 범위**: (0, 1)
- **특징**:
    - 이진 분류 문제에서 사용.
    - 그래디언트 소멸 문제가 발생할 수 있음.
    - 계산 비용이 tanh나 ReLU에 비해 큼.
- **사용 예시**: 로지스틱 회귀, 이진 분류 문제.

---

## 2. Tanh 함수

- **식**: $$f(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}​$$
- **출력 범위**: (-1, 1)
- **특징**:
    - Sigmoid와 유사하지만, 0 중심으로 대칭적.
    - 그래디언트 소멸 문제는 여전히 존재.
- **사용 예시**: 음수 값이 중요한 문제.

---

## 3. ReLU (Rectified Linear Unit)

- **식**: $$f(x) = \max(0, x)$$
- **출력 범위**: [0, ∞)
- **특징**:
    - 그래디언트 소멸 문제를 완화.
    - 계산이 간단하지만 죽은 뉴런 문제가 발생할 수 있음.
- **사용 예시**: 대부분의 딥러닝 모델.

---

## 4. Leaky ReLU

- **식**: $$f(x)={\begin{cases} x & \text{if } x > 0 \\ \alpha x & \text{if } x \leq 0 \end{cases}}$$
- **출력 범위**: (-∞, ∞)
- **특징**:
    - ReLU에서 죽은 뉴런 문제를 완화.
    - 음수 값도 학습에 기여.
- **사용 예시**: ReLU의 대안.

---

## 5. ELU (Exponential Linear Unit)

- **식**: $$f(x)={\begin{cases} x & \text{if } x > 0 \\ \alpha (e^x - 1) & \text{if } x \leq 0 \end{cases}}$$
- **출력 범위**: (-α, ∞)
- **특징**:
    - 음수 입력에서 부드러운 변화 제공.
    - 안정적인 학습 가능.
- **사용 예시**: 안정적인 수렴이 필요한 모델.

---

## 6. Softmax

- **식**: $$f(xi)= \frac{e^{x_i}}{\sum_{j} e^{x_j}}​$$​
- **출력 범위**: (0, 1)
- **특징**:
    - 출력값을 확률로 해석 가능.
    - 전체 출력값의 합은 1로 정규화됨.
- **사용 예시**: 다중 클래스 분류.

---

## 7. Swish

- **식**: $$f(x) = x \cdot \text{sigmoid}(\beta x)$$
- **출력 범위**: (-∞, ∞)
- **특징**:
    - Google에서 제안한 활성화 함수.
    - ReLU보다 성능이 우수한 경우가 많음.
- **사용 예시**: 최신 딥러닝 연구에서 사용.

---

## 8. GELU (Gaussian Error Linear Unit)

- **식**: $$f(x) = x \cdot \frac{1}{2} \left[ 1 + \text{erf}\left(\frac{x}{\sqrt{2}}\right) \right]$$
- **출력 범위**: (-∞, ∞)
- **특징**:
    - ReLU보다 정교한 결과 제공.
    - NLP 모델(특히 트랜스포머 기반 모델)에서 많이 사용.
- **사용 예시**: BERT, GPT 등.

---

# 비교표

| 함수         | 출력 범위   | 주요 문제 해결    | 특징/단점             | 사용 예시         |
| ---------- | ------- | ----------- | ----------------- | ------------- |
| Sigmoid    | (0, 1)  | 없음          | 그래디언트 소멸 문제 발생 가능 | 이진 분류         |
| Tanh       | (-1, 1) | 없음          | 0 중심, 그래디언트 소멸 가능 | 음수값도 중요한 경우   |
| ReLU       | [0, ∞)  | 그래디언트 소멸 완화 | 죽은 뉴런 문제          | 대부분의 딥러닝 모델   |
| Leaky ReLU | (-∞, ∞) | 죽은 뉴런 완화    | 음수 값의 작은 기여       | ReLU 대안       |
| ELU        | (-α, ∞) | 안정적 학습      | 계산 복잡성 증가         | 안정적인 수렴 필요 모델 |
| Softmax    | (0, 1)  | 다중 클래스      | 확률 기반 출력          | 다중 클래스 분류     |
| Swish      | (-∞, ∞) | ReLU 대안     | 부드러운 그래디언트, 높은 성능 | 최신 연구         |
| GELU       | (-∞, ∞) | NLP 최적화     | 수학적 복잡성           | 트랜스포머 기반 모델   |

### **그라디언트 소멸 문제 (Vanishing Gradient Problem)**

#### **개념**

그라디언트 소멸 문제는 **딥러닝에서 역전파(Backpropagation)** 과정 중, 네트워크의 깊은 층으로 갈수록 **그래디언트(Gradient)** 값이 점점 작아져서 결국 거의 0에 수렴하는 현상.

이로 인해서 정확한 학습이 어려워 진다.
[참고 사이트](https://velog.io/@lighthouse97/%EA%B8%B0%EC%9A%B8%EA%B8%B0-%EC%86%8C%EC%8B%A4-%EB%AC%B8%EC%A0%9C%EC%99%80-ReLU-%ED%95%A8%EC%88%98)

간단하게 정리하면
시그모이드 함수의 최대 기울기는 0.25 고 최소는 0 이다 
이때 시그모이드 함수를 여러번 사용하면 기울기가 1보다 작기 때문에
점점 역전파 하는 값이 줄어들다가 결국 학습되는 값이 0에 가까워저 학습이 일어나지 않는다는 거다.

따라서 이떄 ReLU함수를 쓰게된다.
ReLU함수의 장점은
- 학습이 빠르다
- 그라디언트 소멸 문제가 발생하지 않는다
단점은
- 음수가 없다 그로인해 죽은 **출력이 항상 0**이 되는 뉴런들이 학습에서 영구적으로 비활성화되는 "죽은 ㅠㅠ뉴ㅗㅗ런 문제(dead neuron problem)뉴런 현상이 발생해 아무런 일도하지않는 뉴런이 발생할수 있다.
