로지스틱 회귀는 이진분류 문제를 해결하기 위해서 사용한다.
즉, 결과가 예 아니요 인 경우 사용한다.

이진 분류에서 고양이 사진을 찾으려 하면
고양이 사진이 1
그외 사진이 0 
이된다.

로지스틱 함수는 시고모이드 함수와 동일하다.

시그모이드는 인공신경망을 거치고 나온 값을 0~1 사이에 갑으로 만든다


Odds (오즈):

- 성공 확률(p)을 실패 확률(1-p)로 나눈 값입니다
- odds = p/(1-p)
- 예: 성공확률이 0.8이면, odds = 0.8/0.2 = 4 (실패 대비 성공할 확률이 4배)

신경망 출력을 q
정답을 y
이라고 할떄

q = 1 (신경망이 예측한 값) 이 1(y = 1)이 나오게 하지 = "q값을 키우자"
q = 0 (신경망이 예측한 값) 이 0(y = 0)이 나오게 하지 = "1 - q값을 키우자"
=> q^y * (1 - q)^(1-y)
위 2개의 정의를 아레처럼 한줄로 바꿀수있다.
 q(1)^y(1) * (1 - q(1))^(1-y(1)) : 첫번째 사진에 대해서 AI가 예측한 해당 동물일 확률

(q(1)^y(1) * (1 - q(1))^(1-y(1))) * (q(2)^y(2) * (1 - q(2))^(1-y(2))) * (q(3)^y(3) * (1 - q(3))^(1-y(3))) ...
=> log (
(q(1)^y(1) * (1 - q(1))^(1-y(1))) * (q(2)^y(2) * (1 - q(2))^(1-y(2))) * (q(3)^y(3) * (1 - q(3))^(1-y(3))) ...
)
\=  log(q(1)^y(1) * (1 - q(1))^(1-y(1))) + log(q(2)^y(2) * (1 - q(2))^(1-y(2))) + log(q(3)^y(3) * (1 - q(3))^(1-y(3))) ...
\= 

---
미니 배치 방식으로 학습할때 각각의 시행은 독립 시행임으로
곱해서 계산한다. (독립시행시 확를 계산 방식 검색)

하지만 최대값이 1임으로  결국 곱해서 나온 값은 수가 많아질수록 점점 0에 수렴하게 됨
-> 언더 플로우

따라서 log를 취해서 곱을 합으로 바꿔서 계산 하게된다.

 log를 취하면 곱이 합으로 되는 이유: log 곱샘법칙 지수 계산 방식 검색

 log를 취할수 있는 이유: 결국 이걸 계산 하는 이유는 학습하기 위해서 학습의 방향과 정도를 계산 하는건데 로그함수는
 