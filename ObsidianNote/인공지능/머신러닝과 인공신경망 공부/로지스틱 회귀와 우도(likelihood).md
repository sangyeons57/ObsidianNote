로지스틱 회귀는 이진분류 문제를 해결하기 위해서 사용한다.
즉, 결과가 예 아니요 인 경우 사용한다.

이진 분류에서 고양이 사진을 찾으려 하면
고양이 사진이 1
그외 사진이 0 
이된다.

로지스틱 함수는 시고모이드 함수와 동일하다.

시그모이드는 인공신경망을 거치고 나온 값을 0~1 사이에 갑으로 만든다


Odds (오즈):

- 성공 확률(p)을 실패 확률(1-p)로 나눈 값입니다
- odds = p/(1-p)
- 예: 성공확률이 0.8이면, odds = 0.8/0.2 = 4 (실패 대비 성공할 확률이 4배)

신경망 출력을 q
정답을 y
이라고 할떄

q = 1 (신경망이 예측한 값) 이 1(y = 1)이 나오게 하지 = "q값을 키우자"
q = 0 (신경망이 예측한 값) 이 0(y = 0)이 나오게 하지 = "1 - q값을 키우자"
=> q^y * (1 - q)^(1-y)
위 2개의 정의를 아레처럼 한줄로 바꿀수있다.
 q(1)^y(1) * (1 - q(1))^(1-y(1)) : 첫번째 사진에 대해서 AI가 예측한 해당 동물일 확률

(q(1)^y(1) * (1 - q(1))^(1-y(1))) * (q(2)^y(2) * (1 - q(2))^(1-y(2))) * (q(3)^y(3) * (1 - q(3))^(1-y(3))) ...
=> log (
(q(1)^y(1) * (1 - q(1))^(1-y(1))) * (q(2)^y(2) * (1 - q(2))^(1-y(2))) * (q(3)^y(3) * (1 - q(3))^(1-y(3))) ...
)
\=  log(q(1)^y(1) + (1 - q(1))^(1-y(1))) + log(q(2)^y(2) + (1 - q(2))^(1-y(2))) + log(q(3)^y(3) + (1 - q(3))^(1-y(3))) ...
\=
$$\ell(\theta) = \log L(\theta) = \log \prod_{i=1}^n P(x_i|\theta) = \sum_{i=1}^n \log P(x_i|\theta)$$
$$\ell(\theta) = \sum_{i=1}^n \left[y(i)\log(q(i)) + (1-y(i))\log(1-q(i))\right]$$


---
미니 배치 방식으로 학습할때 각각의 시행은 독립 시행임으로
곱해서 계산한다. (독립시행시 확를 계산 방식 검색)

하지만 최대값이 1임으로  결국 곱해서 나온 값은 수가 많아질수록 점점 0에 수렴하게 됨
-> 언더 플로우

따라서 log를 취해서 곱을 합으로 바꿔서 계산 하게된다.

 log를 취하면 곱이 합으로 되는 이유: log 곱샘법칙 지수 계산 방식 검색

 log를 취할수 있는 이유: 결국 이걸 계산 하는 이유는 학습하기 위해서 학습의 방향과 정도를 계산 하는건데 로그함수는 단조 증감함수 이기때문이다.
 단조증가란 x 가 증가할때 x가 증가하고 y가 감소할때 같이 감소하는 
 그런 로그 를 적용해도 같은 용도 (학습의 방향성과 정도) 로 사용할수있기때문에 적용해도 아무런 문제가 안된다.
 왜냐하면 학습할떄 도 방향성은 중요하지만 학습의 정도는 계수를 추가로 부처서 조정하는 것 처럼 임의로 설정하기 때문에.

단조 증가 함수는 단조롭지 않더라도
한정된 범위에서 사용될떄 (시그 모이드 함수처럼 1부터 0 사이값)
그안에서 단조롭다면 사용가능하다.



log-odds 
L = log(e)(q / (1 - q)) 
e^L = q / (1 - q)
e^-L = (1- q) / q
e^-L = 1 / q - 1
q = 1 / (e^-L + 1)
시그모이드 함수

따라서 L에 인공 신경망에서 계산 한 값이 들어오고
q가 확률이 된다.

이떄 마지막 시그모이드 함수는 인공신경망의 일부가 아니라 
인공신경망에서 나온 값을 판단,학습 하기위한 값을 구하는 수식 이라고 볼수있다.

