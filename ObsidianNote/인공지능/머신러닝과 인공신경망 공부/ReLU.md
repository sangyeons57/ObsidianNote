시그모이드 함수에 문제가있어 서 세롭게 나온 방식
시그모이드 함수에 문제점
미분했을때 최대 값이 0.25 임
즉 뒤로 전파될수있는 최대값이 0.25 인거임 만약 신경망이 딥 해진다면
0.25 * 0.25 * 0.25 * 0.25 * 0.25 = 1 / 128 이렇게 5개의 은닉층만들어가도
0.01 아레로 내려가서 업데이트 가안됨 또한 앞부분에 영향을 적게미침

따라서 세롭게 나온 ReLU 란는 방식이 있음
이것은  0 이하의 값은 전부 0 을 반환 하고
양수는 y = x 식을 이용하는 거다.
전체 식을 y = x로 표현 하지 않는 이유는 선형적이게 되면 신경망 전체가 선형적이게 되어
비선형적이 것을 표현 못하기 때문이다.
