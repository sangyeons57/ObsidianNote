소프트 맥스 회귀는 다중 클레스 분류문제를 해결하기 위해 설계된 알고리즘이다.

예를들어 사진을 받아 이 사진이 강아지인지, 고양이 인지, 소인지 판단해야할때 사용하는 알고리즘이다.

스프트 맥스 회귀는
만약에 3개의 선택지 가있을떄
1 0 0 / 0 1 0 / 0 0 1 처럼 선택지의 확률을 전부 합했을떄 1이 되어야하고 
정답을 표시한떄는 위처럼 선택된 값만 1이 된다.

소프트 맥스는 
$$
softmax(z)_i = \frac{e^{z_i}}{\sum_{j=1}^{3} e^{z_j}}
$$
위 식처럼
$$
e^{l_1} + e^{l_2} + e^{l_3}
$$
이런 형태로전체값을 구하고

$$
e^{l_1}
$$
로 그중 한가지 값을 구해서

해당 1번쨰 값이 전체의 얼만큼을 차지하는지 측정한다.


소프트 맥스 회귀가 모델의 예측에 사용된다면
아레 크로스 엔트로피는 정확도를 평가하는 데 사용한다.
$$
L = -\sum_{i=1}^{N} y_i \log(\hat{y}_i)
$$
이식이만들어지는 과정은 [[로지스틱 회귀와 우도(likelihood)]] 와 비슷 한데

$$
q^{y_1} q^{y_2} q^{y_3}
$$
이런식으로 예측값을 구할수있다
이떄 y1, y2, y3에 들어가는 것은 위에 나오는 1, 0, 0 같은 정답값을 의미한다
그러면 정답이 되는값만 나와서 likelihood처럼 로스를 구할수 있게 된다.

그리고 여기에  -log를 취하게 되면
곱 형태로 이루어진 식이 로그의 곱셈법칙에 따라 합형태를 띄게 된다.  이떄 log를 취할수 있는 이유는  위에 링크된것 처럼 단조 증가 함수 이기 때문이다.

또한 원 핫 인코딩(1,0,0 처럼 옳은것을 표시 하는 방식) 이기 때문에 한쪽에 오차만 줄이면 나머지 오차는 자동으로 줄어 들기때문에 오차 는 정답에 해당하는 쪽에만 적용해 주는 것 처럼 된다.


소프트 멕스가 로스값을 구하는 도구로 사용해서
크로스 엔트로피에 q값에 소프트 멕스가 적용된 값을 넣어서 신경망에서 소프트 맥스를 분리하는 경우가 많기도 하다.


결국 인공지능은
1. 입, 출력을 정의하고
2. 모델을 만들고
3. 로스를 정의하고
4. wights 를 최적화
허눈 과정을 거친다.


시그모이드에 비해 소프트 맥스의 다중분류에서 장점은
1. Saturation문제가 없다
2. 상대 평가가 가능하다.
3. exponentional fuction(e^x 자연상수 함수)을 사용해서 더욱 두드러진게 표현 할수 있다.


### 엔트로피
정보를 표현 하는데 필요한 최소 자원량
 정보이론 에서 확률변수의 불확실성(uncertainty)을 정량화하는 척도이다.


카카오톡 메시지를 비트로 표현할떄 어떤 매시지가 올지 모르지만
메시지 에서 나올 확률이 높은 놈은적은 비트수로 표현하고
나올 확률이 낮은 놈은 큰 비트 수로 표현하면

이것이 기대값의 최소 라고 할 수 있다.

이떄 유니폼 하게 모든 메시지가 나올 확률이 동일하면
이것을 최악의 확률 이라고 할 수 있다.

$$
H(X) = -\sum_{i} P(x_i) \log_2 P(x_i)
$$

앤트로피는 위 식으로 표현 한다.

먼저 정보량 은 아레 식으로 표현 할 수 있다.

$$
I(X) = -\log_2 P(x_i)
$$
해석 하면

$$
P(x_i)
$$
는 사건 X_i가 발생할 확률을 의미한다.
이떄 log(2) 를 쓰는 이유는 컴퓨터에서 정보를 저장하느 방식이 비트 이기 때문이다.
따라서 1비트당 2개를 표현 할 수 있다.

또한 -log는 p(x)가 0~1 사이를 표현 하기 때문에다.

이때 앤트로피의 계산은 모든 가능한 사건에 대해 평균적인 정보량을 계산 하는 것 을 의미한다.

$$
H(X) = -\sum_{i} P(x_i) I(x_i) = -\sum_{i} P(x_i) \log_2 P(x_i)
$$
따라서 위 식을 계산 할 수 있는데
이땐 P(x_i) 는 해당 정보량이 나타날 확률을 의미한다.
따라서 평균을 구하기 위해서 종류만큼 나눌 필요가 없다 이미 확률에서 나눗셈을 하고 들어간 것 이기 때문에
결국 합하면 1이된다.

크로스 엔트로피 식과 비교하면

$$
L = -\sum_{i=1}^{N} y_i \log(\hat{y}_i)
$$
이걸로 상당히 식이 비슷한 것을 알 수 있는데
보니까 내부적으로느 안비슷한거 같다.


$$
CrossEntropy = H(P) + D_{KL}(P||Q)
$$
이것은 KL발산을 공부해야 한다.

최소저장 방식에 추가하는 방식이다.


크로스 엔트로피와 소프트 맥스 공식을 합친것의 미분은
소프트 맥스 예측값 - 실제 값 이다.
[영상](https://youtu.be/KJJL9_mlMXA?si=ngitVxDiy0Bz5oYg)
