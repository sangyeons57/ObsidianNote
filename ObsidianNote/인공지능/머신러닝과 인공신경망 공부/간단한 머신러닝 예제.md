```python
import numpy as np
import matplotlib.pyplot as plt

#데이터 생성
x = np.array([1,2,3,4,5])
y = np.array([2,3,7.5,9,8.5])

#초기화
theta_0 = 0 #절편
theta_1 = 0 #기울기
alpha = 0.01 #학습률
iterations = 1000 #반복횟수
m = len(x)

#비용함수
def compute_cost(x, y, theta_0, theta_1):
    h = theta_0 + theta_1 * x
    return ( 1 / (2 * m) ) * np.sum((h - y) ** 2)

#경사 하강법
def gradient_descent(x, y, theta_0, theta_1, alpha, iterations):
    for _ in range(iterations):
        h = theta_0 + theta_1 * x
        theta_0 -= alpha * (1 / m) * np.sum(h - y)
        theta_1 -= alpha * (1 / m) * np.sum((h - y) * x)
    return theta_0, theta_1

# 학습
theta_0, theta_1 = gradient_descent(x, y, theta_0, theta_1, alpha, iterations)

# 최종 비용 계산
final_cost = compute_cost(x, y, theta_0, theta_1)

# 결과 출력
print(f"최종 기울기(theta_1): {theta_1}")
print(f"최종 절편(theta_0): {theta_0}")
print(f"최종 비용 함수 값: {final_cost}")

# 시각화
plt.scatter(x, y, color='red', label='Training Data')  # 실제 데이터
plt.plot(x, theta_0 + theta_1 * x, color='blue', label='Prediction')  # 모델
plt.xlabel('x')
plt.ylabel('y')
plt.legend()
plt.show()
```

#### 비용 함수를 보면 평균 제곱 오차 라는 것을 알수있다.
h 는 기울기(theta_1)과 절편(theta_0) 을 이용해 예측 값을 구하고
실제 값과 빼서 (h - y) 제곱을 한후 ((h - y ) ** 2) 전부 합한다. (np.sum())
그후 1 / 개수(m) 을 곱해줘서 평균을 구하게 된다.
그런데 이때 1 / 2m 이 되는 이유는 편미분을 할떄 제곱 부분이 미분 되면서
\* 2 가 됨으로 계산의 복잡성을 없에기 위해 사용한다고 한다.
실제로 프로그램 실행에 영향을 안 미치기도 한다.

$$
비용함수 :
J(\theta_0, \theta_1)
$$$$
J(\theta_0, \theta_1) = \frac{1}{2m} \sum_{i=1}^{m} \left( h(x^{(i)}) - y^{(i)} \right)^2
$$



#### 오차 측정및 감소에는 경사하강법이 사용되었다.
주어진 iterations에 따라 반복을 하면서
예측값 h를 계산하고
오차 ( 1 / m ) * np.sum(h - y) 를 계산한다. **편미분 계산의 목적**은 **오차를 기반으로 파라미터를 업데이트하여 모델을 개선하기 위함**이다.
이떄  (1 / 2 * m) 이 아닌 이유는, 이전에 오차 계산 식에서 편미분을 하기 쉽게 분모에 2를 곱했기 때문이다.
그후에  학습률 (alpha) 를 곱해서 theta_0 -= 을 통해 학습에 적용 하게 된다.


절편을 구할떄 x값을 곱하는 이유는
먼저 x는 각 인풋 마다 모두달라서 곱해서 영향을 주는 theta_1은 모든 다르게 영향 받지만
theta_0 절편은 모두에게 같게 영향을 준다. 
자세한건 오차역전파 영상에서 확인 할수있다.

